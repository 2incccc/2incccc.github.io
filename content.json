{"meta":{"title":"Blog by 2inc","subtitle":"  ","description":"永远相信美好的事情即将发生","author":"zinan2inc","url":"https://2incccc.github.io","root":"/"},"pages":[{"title":"关于我","date":"2023-01-15T13:13:40.000Z","updated":"2025-02-20T10:03:02.876Z","comments":true,"path":"about/index.html","permalink":"https://2incccc.github.io/about/index.html","excerpt":"","text":"欢迎来到我的博客 Welcome To My Blog INFJ，希望是一个能够自洽的人。爱生活，爱家人，爱自己。 目前就读于北京邮电大学电子信息工程专业，主要研究兴趣围绕网络虚拟化、云计算相关，也有在努力学习算法刷题😭业余热衷一切与音乐有关的东西，时常希望自己还能多发展一些其他领域的爱好，成为一名有内涵的人。 可能是理想主义者… Contact 大部分的社交方式也列举在博客主页中（会持续更新）。欢迎大家选择喜欢的方式联系我。 关于名字，我可以是2inc，可以是zync，可以是zinan2inc，可以是zynestro，未来或许还有更多可能，我也常常随便用几个字母作为我小号的马甲，毕竟名字只是代号罢了（笑 Email📩 zinan2inc@163.com 这是我最主要使用的邮箱地址，应该会长期使用，欢迎联系我。 Github🧑‍💻 2incccc 我的个人仓库主页，希望自己可以多写代码，沉淀… bilibili : 青草木阳菜 音乐区/抽象区随缘更新，希望更多时间花在自己热爱的事情。 - 小红书 - 知乎 - 微信公众号 - 豆瓣 - 网易云音乐 - 微信读书"},{"title":"categories","date":"2023-01-16T05:48:55.000Z","updated":"2025-01-27T07:13:44.067Z","comments":false,"path":"categories/index.html","permalink":"https://2incccc.github.io/categories/index.html","excerpt":"","text":""},{"title":"","date":"2025-01-27T07:13:44.067Z","updated":"2025-01-27T07:13:44.067Z","comments":true,"path":"css/custom.css","permalink":"https://2incccc.github.io/css/custom.css","excerpt":"","text":":root { --first-screen-font-color-light: #edf9e5; --first-screen-font-color-dark: #ffffff; --first-screen-icon-color-light: #e4eadc; --first-screen-icon-color-dark: #ffffff; --first-screen-header-font-color-light: #78fa22; --first-screen-header-font-color-dark:#ffffff; }"},{"title":"","date":"2025-01-27T07:13:44.092Z","updated":"2025-01-27T07:13:44.092Z","comments":true,"path":"js/custom.js","permalink":"https://2incccc.github.io/js/custom.js","excerpt":"","text":"if (!window.runningTime) { window.runningTime = () => { const infoBox = document.querySelector('.footer .website-info-box') const tempDiv = document.createElement('div'); tempDiv.setAttribute('class', 'info-item default') infoBox.appendChild(tempDiv) const since = '2022-08-16 00:00:00' const formatTimestamp = (timestamp) => { const now = Date.now() const timeDiff = Math.abs(now - timestamp) const days = Math.floor(timeDiff / (1000 * 60 * 60 * 24)) const hours = Math.floor((timeDiff % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60)) const minutes = Math.floor((timeDiff % (1000 * 60 * 60)) / (1000 * 60)) const seconds = Math.floor((timeDiff % (1000 * 60)) / 1000) return `${days} 天 ${hours} 小时 ${minutes} 分 ${seconds} 秒` } setInterval(() => { tempDiv.innerHTML = '本站已安全运行 ' + formatTimestamp(new Date(since).getTime()) }, 1000) } } window.runningTime()"},{"title":"links","date":"2024-07-24T05:06:06.000Z","updated":"2025-01-27T07:13:44.092Z","comments":true,"path":"links/index.html","permalink":"https://2incccc.github.io/links/index.html","excerpt":"","text":"若要交换友链，请先将本站添加到你的友链列表里，然后按下列&lt;&lt;示例格式&gt;&gt;留言： 1234- name: # 博主名 link: # 博客链接 description: # 描述 avatar: # 头像，提供图床链接"},{"title":"photos","date":"2024-07-13T07:16:00.000Z","updated":"2025-01-27T07:13:44.092Z","comments":true,"path":"photos/index.html","permalink":"https://2incccc.github.io/photos/index.html","excerpt":"","text":"即将更新…"},{"title":"关于站点","date":"2023-01-17T13:13:40.000Z","updated":"2025-02-20T10:07:04.942Z","comments":true,"path":"site/index.html","permalink":"https://2incccc.github.io/site/index.html","excerpt":"","text":"2022年暑 偶然了解到hexo博客框架，恰好也有兴趣在网络上搭建一个记录分享自己学习、生活的空间，（也是为激励自己不断进步），便由此开始了折腾的过程。 hexo博客搭建极易上手，但是折腾人的地方在于对主题文件的配置以及自定义。出于对美感(花里胡哨)的追求，至今（2023/1/16）我尝试更换了多种主题butterfly NexT Fluid Volantis Nexmoe matery 等等， 最终到现在在用的 Yun。 2023年寒 目前该主题配置基本已完成，下一步将尝试搭建可靠的图床方便图片加载（github图床的jsDelivr感觉还是慢，本地图床又太折腾），当然也会慢慢补全先前的笔记文章等，扩充博客内容。此外，由于关于博客部分目前内容尚少，随着时间推移内容扩充后会考虑新建关于站点的超链接并在侧边栏重新渲染。–1/16 完成Yun主题的基本配置----1/16 由于LeveRE 评论系统自带广告，决定更换到waline系统，采用LeanCloud作为数据库，Vercel用于部署服务端----1/17 借助instantlogodesign设计logo,更新网站log----1/20 2023年暑 过去的一个学期博客被我咕咕咕了，暑假来了有时间，该及时更新力 更新过去的一学期里的数篇文章，调整博客和个人的关于页面----6/23 2024年暑 一年啊一年，似乎已经习惯了以年为单位更新了（bushi 更换主题为更简洁的keep，也以此名字激励自己接下来的时间要保持更新。 针对更新后的主题，修改了部分标签页的配置。 Now 没什么心力在折腾花哨的东西了，（也是因为对目前的外观还算满意），未来希望可以专注于内容上。 #TODO: 配置博客的相册页 #TODO: 更新过去一年的博客内容 #TODO: 解决inject特性无法使用的问题，以实现自定义样式 #TODO: 更新个人页面联系方式"},{"title":"tags","date":"2023-01-15T13:16:34.000Z","updated":"2025-01-27T07:13:44.092Z","comments":true,"path":"tags/index.html","permalink":"https://2incccc.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"OSPF 协议原理","slug":"ospf-scratch","date":"2025-02-18T17:23:05.000Z","updated":"2025-02-20T09:51:48.422Z","comments":true,"path":"2025/02/19/ospf-scratch/","link":"","permalink":"https://2incccc.github.io/2025/02/19/ospf-scratch/","excerpt":"上学期接触的算力路由项目涉及对ospf的改造，这里是对ospf协议学习内容的留档","text":"OSPF 优点 支持 CIDR，路由聚合更灵活 采用组播形式收发报文，减少对非 ospf 路由器的影响 对于需要洪范的信息，目标 ip 采用 224.0.0.0/24 内的，（5/6），就实现只发向 ospf 路由器 负载分担 报文加密 基础概念 Router ID 路由器标号 可手动配置 从 loopback 地址自动获取 链路状态 对接口之间的关系的描述 接口的信息 IP 地址 掩码 网络类型 邻居 COST cost 是接口的 cost，因为只有接口才有带宽 链路通过的接口相加得到最终的接口 报文类型 Hello 建立邻居关系 DD 描述 LSDB 摘要 LSR 请求 LSA 的更新 LSU 更新 LSA LSAck 确认收到 LSA 类型 Type1 Router-LSA 描述单个设备的链路状态与开销 其实只与接口相关 Type1 的 LSA 内可以包含多个链路的信息，但是必须是属于一个区域的，因为 Type1 的 lsa 只在区域内洪范 Type1 的 1 是指链路状态类型 链路本身也有类型，和网络类型相关 Type2 Network-LSA 由 DR 产生，描述本网段的链路状态 用于 MA 网络，因为路由器的链路都是和 DR 的，不知道其他信息 重点描述 Router ID，拓扑关系 Type3 Network-summary LSA 跟上一个有点类似，但是是在区域边界 这时候会涉及路由聚合 向另一个网络描述 实际上是在区域内 D 算法运行过后进行的 为防止回环，所有区域必须和骨干区相连 Type4 ASBR summary 描述从 abr 到 asbr 的路由，告知给其他相关区域 最先由和 asbr 同区域的 abr 开始洪范 abr 到 asbr 之间的路径已经计算完成，已经得到最短的cost 一旦 asbr 执行引入外部路由，R4 就宣告自己是 asbr，通过 Type 4 描述这个asbr Type5 external 从 asbr 到外部的路由，告知到所有的区域 不包括 stub，是为了避免路由的复杂 Type7 同上，但是设了一个特殊情况，仅在 nssa 区域传播 Type9/10/11 9:在接口所在网段范围内传播 10:在区域内传播 11:在自治域传播 Stub 区域和 NSSA 区域 避免路由表过大，LSA占用过多带宽，对外部路由进行了限制 NSSA 区域允许了 Type 7 lsa，特例 Stub 隔绝 AS，Totally Stub 隔绝 Area 翻译：存根区域 Not So Stubby Area Stub 区域主要配置在 AS 边界，只有一个 ABR 的非骨干区域 以缺省路由代替外部路由 路由器类型 Internal Router Area Border Router Backbone Router AS Boundary Router 路由类型 Intra 区域内 Inter 区域间 Type 1 External 不那么远的路由 Type 2 External 远的路由，不考虑到 asbr 的距离 支持的网络类型 广播类型 Broadcast Multi Access BMA 非广播 None-BMA 点到点 点到多点 邻居和邻接 邻居指建立 Hello 关系 邻接指交换完 DD 和 LSA","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://2incccc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"OSPF","slug":"OSPF","permalink":"https://2incccc.github.io/tags/OSPF/"},{"name":"算力路由","slug":"算力路由","permalink":"https://2incccc.github.io/tags/%E7%AE%97%E5%8A%9B%E8%B7%AF%E7%94%B1/"}]},{"title":"如何使用CMake构建JUCE项目（不使用Projucer）","slug":"juce-cmake","date":"2025-01-27T07:37:06.000Z","updated":"2025-01-27T07:57:33.592Z","comments":true,"path":"2025/01/27/juce-cmake/","link":"","permalink":"https://2incccc.github.io/2025/01/27/juce-cmake/","excerpt":"","text":"最近对VST开发比较感兴趣，发现了JUCE框架，但是JUCE提供的 Projucer 大多配合 Xcode 使用，本人更习惯使用VS Code，又了解到 Projucer 其实只是相当于 CMake 构建项目的 GUI 界面，所以在下面介绍一种VS Code 可以使用的 JUCE 项目构建方法。 JUCE 是一个流行的 C++ 框架，广泛用于音频和 GUI 应用的开发。通常，JUCE 提供的 Projucer 可用于生成工程文件，但如果你希望使用 CMake 直接构建 JUCE 项目，那么本文将指导你如何完成这一过程。 1. 环境准备 在开始之前，请确保你的开发环境满足以下条件： JUCE 库：已下载并解压 JUCE 源码 CMake：已安装 CMake（推荐 3.15 及以上版本） 编译工具链：例如 GCC、Clang，或 Windows 上的 Visual Studio 编译工具 2. 创建 CMakeLists.txt 在你的 JUCE 项目根目录下，创建 CMakeLists.txt 文件，并填入以下内容： 1234567891011121314151617181920212223# 指定 CMake 版本cmake_minimum_required(VERSION 3.15)# 定义项目名称project(MyJUCEApp)# 设置 JUCE 库路径（修改为你的 JUCE 目录）set(JUCE_DIR /path/to/juce)# 引入 JUCEadd_subdirectory($&#123;JUCE_DIR&#125; JUCE)# 定义可执行文件add_executable(MyJUCEApp Source/Main.cpp Source/MainComponent.cpp)# 链接 JUCE 库target_link_libraries(MyJUCEApp PRIVATE juce::juce_gui_basics juce::juce_audio_basics)# 设置 C++ 标准set_target_properties(MyJUCEApp PROPERTIES CXX_STANDARD 17) 注意： 修改 JUCE_DIR：请替换 /path/to/juce 为你的实际 JUCE 路径。 添加更多 JUCE 模块：如果你的项目需要更多模块，可以在 target_link_libraries 里添加，例如 juce::juce_audio_processors。 3. 配置和编译项目 接下来，按照以下步骤编译项目： 创建构建目录 12mkdir buildcd build 运行 CMake 配置 1cmake .. 编译项目 1cmake --build . 如果一切顺利，你的可执行文件将被生成在 build 目录中。 4. 运行程序 编译完成后，你可以直接运行生成的可执行文件，例如： 1./MyJUCEApp 在 Windows 上，你可以双击 MyJUCEApp.exe 或使用命令行运行。 5. 其他配置 JUCE 模块选择 根据你的需求，添加适当的 JUCE 模块，例如： 1target_link_libraries(MyJUCEApp PRIVATE juce::juce_gui_basics juce::juce_audio_processors) 平台特定设置 如果你的项目需要在多个平台上运行，可以添加不同的配置： 1234567if(WIN32) # Windows-specific settingselseif(APPLE) # macOS-specific settingselseif(UNIX) # Linux-specific settingsendif() 包含自定义模块 如果你有自己的模块，可以使用 add_subdirectory 或 include_directories 进行添加。 总结 本文介绍了如何使用 CMake 构建 JUCE 项目，完全绕过 Projucer，并展示了如何配置 CMakeLists.txt 以及执行编译。 如果你在构建过程中遇到问题，欢迎留言讨论！","categories":[{"name":"juce开发","slug":"juce开发","permalink":"https://2incccc.github.io/categories/juce%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"juce","slug":"juce","permalink":"https://2incccc.github.io/tags/juce/"},{"name":"cmake","slug":"cmake","permalink":"https://2incccc.github.io/tags/cmake/"}]},{"title":"基于飞书的个人工作流分享","slug":"spark-workflow","date":"2024-07-24T17:41:14.000Z","updated":"2025-01-27T07:13:44.066Z","comments":true,"path":"2024/07/25/spark-workflow/","link":"","permalink":"https://2incccc.github.io/2024/07/25/spark-workflow/","excerpt":"","text":"","categories":[],"tags":[{"name":"工具","slug":"工具","permalink":"https://2incccc.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"效率","slug":"效率","permalink":"https://2incccc.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"经验分享","slug":"经验分享","permalink":"https://2incccc.github.io/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/"}]},{"title":"ICASSP 2024 论文节选","slug":"icassp-notes","date":"2024-05-24T16:31:58.000Z","updated":"2025-02-20T09:51:48.406Z","comments":true,"path":"2024/05/25/icassp-notes/","link":"","permalink":"https://2incccc.github.io/2024/05/25/icassp-notes/","excerpt":"本篇内容主要为实习期间主要研究的几篇论文的笔记内容，在此留档，主要涉及舞蹈生成方向、音频生成等。","text":"本篇内容主要为实习期间主要学习的几篇论文的笔记内容，在此留档，主要涉及舞蹈生成方向、音频生成等。 @huangEnhancingExpressivenessDance2024 title:Enhancing Expressiveness in Dance Generation Via Integrating Frequency and Music Style Information 提高舞蹈动作的表现力 Abstract 提出了 ExpressiveBailando 针对流派匹配、节拍对齐、舞蹈动态三个方面，提出了衡量表现力 Expressiveness 的要素 流派/节拍：一个预先训练的音乐模型：Mert 动态表现：将频率信息纳入 VQ-VAE,Frequency Complemented VQ-VAE FreqVQ-VAE Inroduction 流派/节拍：意味着舞蹈和音乐的和谐匹配程度 舞蹈动态相对抽象一点， 和表演方式、动作形式联系起来，主要与动作速度有关 Methods 这张架构图展示了 ExpressiveBailando 的总体结构，具体讲解如下： 整体概述 ExpressiveBailando 是一个用于生成高表现力舞蹈的系统。该系统利用音乐特征和舞蹈编码，通过频率补充 VQ-VAE（FreqVQ-VAE）和跨条件 GPT 生成舞蹈。 各部分的详细解释 音乐处理部分： MERT：预训练的音乐模型 MERT 用于提取音乐特征。这些特征包含丰富的音乐风格信息（如类型和节奏）。 CONV：卷积层将 MERT 特征下采样。 Handcrafted music features：手工制作的音乐特征，如 MFCC（梅尔频率倒谱系数），与 MERT 特征一起作为音乐条件输入。 舞蹈处理部分： Upper body FreqVQ-VAE Encoder：对上半身舞蹈序列进行编码，生成上半身姿态编码。 Lower body FreqVQ-VAE Encoder：对下半身舞蹈序列进行编码，生成下半身姿态编码。 Codebook Zu 和 Z：分别保存上半身和下半身的编码字典，每个条目代表一个有意义的舞蹈姿态。 跨条件 GPT： Positional Embedding：将上半身姿态编码、下半身姿态编码与音乐条件输入进行位置嵌入。 Cross-Conditional GPT：根据输入的音乐和初始姿态编码生成未来的上半身和下半身姿态编码（au 和 al）。 Top-1 Selection：选择最有可能的姿态编码。 解码部分： Upper body FreqVQ-VAE Decoder：根据上半身姿态编码生成上半身的舞蹈序列。 Lower body FreqVQ-VAE Decoder：根据下半身姿态编码生成下半身的舞蹈序列。 Future Dance：最终生成的 未来舞蹈序列，由上半身和下半身的舞蹈序列组成。 工作流程 音乐输入到 MERT，提取出音乐特征后通过卷积层下采样，与手工制作的音乐特征一起形成音乐条件输入。 舞蹈输入分别通过上半身和下半身的 FreqVQ-VAE 编码器，生成姿态编码。 将这些编码与音乐条件输入进行位置嵌入，然后输入跨条件 GPT，生成未来的上半身和下半身姿态编码。 根据生成的姿态编码，通过 FreqVQ-VAE 解码器生成未来的舞蹈序列。 目的 这种架构通过结合频率信息和音乐风格信息，改进了舞蹈的类型匹配、节奏对齐和舞蹈动态性，增强了生成舞蹈的表现力。 @liExploringMultiModalControl2024 title:Exploring Multi-Modal Control in Music-Driven Dance Generation 舞蹈动作的多模态控制 Abstract 聚焦于生成过程中的输入信号控制 一个可以实现多模态控制的生成框架 控制和生成是分开的 对于不同类别的信号采取不同的策略 Inroduction 在同一个框架实现多模态控制： 风格控制 基于文本的语义控制 对于关键帧的动作控制 Methods 这张图展示了提出的方法的整体流程，分为预训练、协同训练和推理三个阶段。以下是每个阶段的详细解释： 预训练：VQ-VAE Motion VQ-VAE： 动作片段（Mm 和 Mt）被输入编码器，编码为离散的动作编码（Tokens）。 解码器将编码解码回原始舞蹈动作。 通过这种方式，所有动作片段被转换为离散的动作编码，这些编码表示共享的潜在空间。 协同训练：带控制模块的跨模态 GPT Text2Motion GPT： CLIP：用于提取文本特征 T。 T-Base：文本 Transformer 基础层，用于处理文本特征。 Transformer Head Layer：与音乐到舞蹈 GPT 共享的头层。 训练目标：最大化数据分布的对数似然（Lrecon），以预测动作编码。 Music2Dance GPT： MLP：多层感知器，用于提取音乐特征 M。 M-Base：音乐 Transformer 基础层，用于处理音乐特征。 Transformer Head Layer：与 Text2Motion GPT 共享的头层。 Genre Control：类型嵌入网络（GEN）和多类型判别器，用于实现类型控制。 Mask Attention/Causal Attention：遮掩注意力/因果注意力机制，用于实现关键帧控制。 训练目标：多类型舞蹈判别器（Lgenre），确保生成的舞蹈符合给定的类型。 推理阶段：统一可控舞蹈生成框架 统一框架： Music：输入音乐，通过 MLP 提取音乐特征 M。 Genre Control：通过 GEN 生成类型嵌入 G。 控制模块： M-Base：用于处理音乐特征。 Text Control Module：处理文本特征并进行语义控制。 Mask Attention/Causal Attention：实现关键帧控制或序列生成。 Transformer Head Layer：共享的头层，用于处理特征并预测动作编码。 Decoder：将预测的动作编码解码为舞蹈序列。 详细流程 预训练 Motion VQ-VAE： 将舞蹈动作编码为离散的动作编码，通过解码器重建舞蹈。 协同训练跨模态 GPT： Text2Motion GPT：使用 CLIP 提取文本特征，通过 T-Base 处理，并通过共享的 Transformer Head Layer 预测动作编码。 Music2Dance GPT：使用 MLP 提取音乐特征，通过 M-Base 处理，并通过共享的 Transformer Head Layer 预测动作编码。 多模态控制： 文本控制：融合音乐和文本特征，实现语义控制。 类型控制：通过类型嵌入网络和判别器，实现类型控制。 关键帧控制：通过遮掩注意力机制，实现关键帧控制。 推理阶段： 输入音乐，通过 MLP 提取音乐特征。 通过类型嵌入网络生成类型嵌入。 控制模块处理音乐和文本特征，通过共享的 Transformer Head Layer 预测动作编码。 通过解码器将预测的动作编码解码为舞蹈序列。 这种方法通过解耦舞蹈生成和控制，确保了生成舞蹈的高质量，同时实现了多模态控制，包括类型控制、语义控制和关键帧控制。 liuLearningHierarchicalCrossModal2022 Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation Abstract 研究问题：根据语音生成一致的手势与动作 提出了 Hierarchical Audio-to-Gesture (HA2G) for co-speech gesture generation 用于生成协同语音手势的分层音频到手势 (HA2G) 制定基于音频文本对齐的对比学习策略，以获得更好的音频表示 Inroduction 动作和手势在语言交流传达意思中很重要 传统上，是将语音和动作一一对应下来，效果并不好，更好的是数据驱动的深度学习方法 两个观察结果： 1）不同类型的协同语音手势与不同级别的音频信息相关。例如，隐喻手势与高级语音语义密切相关（例如，在描绘峡谷时，人们会将两只伸出的手分开并说“间隙”），而节拍和音量等低级音频特征则与高级语音语义密切相关。到有节奏的手势。 2）不同人体部位在协同语音手势中的动态模式并不相同，例如灵活的手指和相对静止的上臂。因此，像以前的研究一样生成整个上半身姿势是不合适的 整个框架包括两个部分，Hierarchical Audio Learner, and the Hierarchical Pose Inferer. 分层音频学习器以及分层姿势推断器 分层音频学习器：提取分层音频特征并通过对比学习呈现判别性表示。 分层姿势推断器：学习多级特征和人体部位之间的关联。因此，人体姿势以级联方式生成。 Related Work Human-Centered Audio-Visual Learning：音频-视觉，主要局限在舞蹈生成、面部表情生成中，不如手势生成更复杂 Human Motion Synthesis：人类动作合成：计算机图形学的重要课题 Audio/Text-Driven Motion Generation：建议对文本、音频、说话者身份的三模态特征嵌入进行编码，并将它们连接在一起以传递解码器。… Approach 这张流程图描述了一个语音驱动的手势生成框架。这个框架通过学习语音和手势之间的层次化关系来生成与语音同步的手势动作。流程图可以分为几个主要部分，每个部分执行不同的功能： Hierarchical Audio Learner（分层音频学习器）: 输入语音（a）和文本（t）。 通过不同的神经网络层次来提取语音的不同层次的特征（低、中、高）。 这些特征用于正面和负面样本的对比学习，通过 L_multi 损失函数来优化。 Hierarchical Pose Inferer（分层姿态推断器）: 利用一个编码器 E_ID，它接收视频帧（I）作为输入，提取与身份相关的特征（f_id）。 这些特征与从 Hierarchical Audio Learner 获得的音频特征一起，通过多个 GRU（门控循环单元）网络层来逐步预测姿态。 每个 GRU 层负责生成一组特定的姿态细节，这些层次结构预测从粗糙到细粒度的姿态。 使用 softmax 进行样式采样，以便选择特定的手势样式。 输出: 多个不同层次的预测手势（(\\hat{P}^1) 到 (\\hat{P}^6)）。 这些预测被一起优化，以减少与真实手势之间的差异，使用层次性 Huber 损失函数。 损失函数: L_KLD 和 L_style 用于控制生成的手势的多样性和风格。 L_GAN和L_phy可能用于增强手势的自然性和物理合理性。 总体而言，该框架的目标是利用文本和语音输入来生成与之对应的、自然流畅的手势动作。这个过程涉及深度学习和神经网络，特别是对比学习和循环神经网络，以模拟人类的手势和动作。 tsengEDGEEditableDance2022 EDGE: Editable Dance Generation From Music Abstract 可编辑的舞蹈生成方法 EDGE 使用基于 Transformer 的扩散模型，与强大的音乐特征提取器 Jukebox 配合使用，并赋予非常适合舞蹈的强大编辑功能，包括联合调节和中间处理。 Inroduction 先前研究 音乐生成舞蹈并不是很能让人满意 对生成舞蹈的评估往往是有缺陷的 根据输入音乐创建逼真、物理上合理的舞蹈动作 贡献： 基于 Diffusion 的 EDGE 方法 分析了以前工作的指标，表明不好 使用新颖的接触一致性损失来消除运动中的脚滑动物理不可信行，引入物理足接触分数 利用 Jukebox 的音频特征提取 Related Work 动作生成 早期属于运动匹配的范畴，插值进行操作 深度学习领域，往往会忽略了物理上的真实性 舞蹈生成 遵循动作检索范例 在大量数据集训练 我们提出一个简单目标训练的单一模型 生成扩散模型 生成建模的有效途径 生成以文本为条件的运动方面 以音乐为条件，难度更大 Method 姿势的建摸：24 个关节，每个关节 6 个自由度，和单独的脚步的建模 24* 6+3=147，每只脚，2 个接触标签，共 151 维度 扩散框架： 辅助损失，四种 编辑功能， 固定部分帧，推理其他帧 tsengMusictoDancePoseLearning2024a Music-to-Dance Poses: Learning to Retrieve Dance Poses from Music 从音乐中检索舞蹈 Abstract 🔤EDSA 适配器是一种利用编码器-解码器转换的自注意力适配器，可以有效且高效地对大规模预训练音乐模型进行微调，以学习从音乐片段到 3D 人体姿势和形状的投影。🔤 EDSA 适配器，利用编码器-解码器转换的子注意力适配器，用于微调模型 将预训练的大规模音乐模型微调为能够将音乐片段投影到3D 人体姿态和形状参数上的模型 Inroduction 没有将其看作生成任务，而是看作跨模态的检索任务，输入上一刻动作输出下一刻 将预训练的大规模音乐模型微调为能够将音乐片段投影到3D 人体姿态和形状参数上的模型 EDSA 适配器，利用编码器-解码器转换的子注意力适配器，用于微调模型 相当于直接打通了音乐-&gt;动作的过程 Methods xieEnhancingAudioGeneration2024 Enhancing Audio Generation Diversity with Visual Information利用视觉信息增强音频生成的多样性 Abstract 利用视觉信息 指导音频内容的生成 Inroduction 由文本生成音频引入，TTA DCASE2023 task7 [1] provides a dataset for categorybased audio generation 问题：训练集的音频比模型生成的更多样，原因是更多隐式的特征无法被学习 how 对训练集音频采用无监督聚类方法，得到更细致的分类 为体现 audio-vision 对齐，对其对细致的分类使用互联网上的图像，共同输入进模型 提出了一个新的框架 训练过程： 基于训练集无监督聚类，之后自行添加图片作为输入，将 label 和 image 进行 fusion 得到新的输入，然后将对应的音频经过编码器得到了 audio representation, 模型主要学习如何从融合输入预测 represention。此外，需要用训练集音频训练Vocoder 推理过程，新的融合信息，进入模型得到对应的 represention，然后经过 decoder 得到频谱，然后进入 vocoder 得到音频 Methods Modal Fusion Music Representations VAE VQ-VAE (所以原理是什么？) Token Prediction 基于自回归的(transformer 基于扩散的 LDM Experimental 两种框架 VAE&amp;LDM VQ-VAE&amp;Transformer Results and discussions 评价指标 客观 Quality: FAD，和参考样本的对比 Diversity: Mean Squared Distance 均方距离 主观（评测） Conclusions 基于视觉信息，增强音频生成的质量和多样性 Summary 视觉-&gt;听觉 单纯基于图片生成音频？ 基于视频生成背景音乐？ zhuHumanMotionGeneration2023 Human Motion Generation: A Survey 综述 Abstract 研究范围：基于条件信号（文本、音频、场景）生成人体运动 该领域的首次综述 Inroduction 生成方法：自回归模型、变分自编码器 (VAE) 、归一化流 、生成对抗网络 (GAN) 和去噪扩散概率模型 (DDPM) 建模技术的进步，使得数据集收集更加方便 三个问题（挑战） 动作本身是复杂、非线性的，运动机理复杂，视觉上合理性 要和条件信息相符合一致 注意一些潜在反应内容的因素 章节section梗概 2 介绍范围 3 基础知识介绍 4-6 介绍总结方法 Preliminaries 预备知识 Motion Data Representation 基于关节 基于旋转 SMPL Motion Data Collection 基于标记的 光学标记 传感器标记（动捕） 不基于标记的 借助计算机视觉算法 伪标记 通过 estimator 估计后生成 手动打标 类似 mmd 的原理 Motion Generation Methods 基于回归模型的 监督学习，从给定条件信号构建目标生成动作 基于生成模型的 Generativa Adversarial Networks 生成对抗网络 Variational Autoencoders 变分自动编码器 Normalizing Flows 归一化流 Diffusion Models 扩散模型 Motion Graph 运动图 Methods Text-Conditioned Motion Generation Action to Motion 根据特定的动作类别生成人体动作 往往擅长但动作的运动，多动作复杂序列比较困难 Text to Motion 根据不同的文本描述到更广泛的动作 Audio-Conditioned Motion Generation music to dance 一种方法是直接基于全监督的回归模型，但是多样性缺乏 基于生成模型的方法，GAN/diffuion/VAE/运动图 长时间序列的舞蹈动作生成 Speech to gesture 根据语音音频生成上班深动作，聚焦于人的手势，在交流中发挥重要作用 言语手势存在显著人际差异，没有较好的普遍性 Scene-Conditioned Motion Generation 生成于场景上下文一致的合理人体运动，是计算机图形学和计算机诗句恶的长期存在的问题。 除去动态动作，还包括静态姿势","categories":[],"tags":[{"name":"论文","slug":"论文","permalink":"https://2incccc.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"笔记","slug":"笔记","permalink":"https://2incccc.github.io/tags/%E7%AC%94%E8%AE%B0/"},{"name":"Dance Generation","slug":"Dance-Generation","permalink":"https://2incccc.github.io/tags/Dance-Generation/"},{"name":"Motion Generation","slug":"Motion-Generation","permalink":"https://2incccc.github.io/tags/Motion-Generation/"}]},{"title":"经典网络架构","slug":"classic-architecture","date":"2024-03-12T17:05:59.000Z","updated":"2025-02-20T09:51:48.414Z","comments":true,"path":"2024/03/13/classic-architecture/","link":"","permalink":"https://2incccc.github.io/2024/03/13/classic-architecture/","excerpt":"","text":"多层感知机 多层感知机的简洁实现 123456789101112131415161718192021222324252627import torchfrom torch import nnfrom d2l import torch as d2l # 多层感知机的简洁实现import torchfrom torch import nnfrom d2l import torch as d2lnet = nn.Sequential(nn.Flatten(), nn.Linear(784,256), nn.ReLU(), nn.Linear(256,10) )# 建立模型def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std = 0.01) net.apply(init_weights);batch_size, lr, num_epochs = 256, 0.1, 10loss = nn.CrossEntropyLoss(reduction='none')trainer = torch.optim.SGD(net.parameters(), lr=lr)train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)&gt;) 模型选择、欠拟合和过拟合 训练误差：训练数据集上计算得到的误差 泛化误差：（对于其他数据集）在无限多数据样本中模型误差的期望 模型选择 训练集：训练模型，获取参数 验证集：调整模型超参数，并选取最佳参数 测试集：验证模型，训练和验证的过程不能使用测试集 当训练数据稀缺，采用 K 折交叉验证，分出 K 个子集，进行 K 次训练，每次使用不同的子集作为验证集，其余 K -1 个子集作为训练集 总结 模型容量需要匹配数据复杂度 统计机器学习提供数学工具来衡量模型复杂度 实际中一般靠观察训练误差和验证误差 神经网络计算 层和块 一个块可以由许多层组成；一个块可以由许多块组成。 块可以包含代码。 块负责大量的内部处理，包括参数初始化和反向传播。 层和块的顺序连接由 Sequential 块处理。 参数管理 访问参数，用于调试、诊断和可视化； 参数初始化； 在不同模型组件间共享参数。 参数是复合的对象，包含属性、值等等 1nn.init.normal_(m.weight, mean=0, std=0.01) # 替换函数 CNN 卷积神经网络 卷积神经网络（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。 6.1 从全连接层到卷积 两个原则：平移不变性、局部性 重新考量全连接层：权重变为四维，输入输出变为矩阵 6.2 图像卷积 6.3 填充和步幅 假设输入形状为 ，卷积核形状为 ，那么输出形状将是()。 因此，卷积的输出形状取决于输入形状和卷积核的形状。 填充 填充多少行，输出多多少行 通常情况，需要填充 行，使得输入和输出维度相同，此时，、 通常为奇数（1 3 5） 步幅 通常为了高效计算或者缩减采样次数 6.4 多输入多输出通道 6.5 汇聚层 也叫池化层 6.6 卷积神经网络（LeNet） 12345678910111213import torchfrom torch import nnfrom d2l import torch as d2lnet = nn.Sequential( nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(), nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10)) Modern CNN 深度卷积神经网络 AlexNet 本质上是更深更大的 LeNet VGG 使用块的网络 网络中的网络 NiN 对每个像素的位置独立使用全连接层 123456789101112131415161718192021222324import torchfrom torch import nnfrom d2l import torch as d2ldef nin_block(in_channels, out_channels, kernel_size, strides, padding): return nn.Sequential( nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, kernel_size=1), nn.ReLU()) net = nn.Sequential( nin_block(1, 96, kernel_size=11, strides=4, padding=0), nn.MaxPool2d(3, stride=2), nin_block(96, 256, kernel_size=5, strides=1, padding=2), nn.MaxPool2d(3, stride=2), nin_block(256, 384, kernel_size=3, strides=1, padding=1), nn.MaxPool2d(3, stride=2), nn.Dropout(0.5),# 标签类别数是10 nin_block(384, 10, kernel_size=3, strides=1, padding=1), nn.AdaptiveAvgPool2d((1, 1)),# 将四维的输出转成二维的输出,其形状为(批量大小,10) nn.Flatten()) 残差网络 ResNet 确保新的映射包含原有的映射，防止网络是退化的 RNN 循环神经网络 循环神经网络可以很好的处理序列信息 序列模型 研究的是序列预测问题： 两个策略：自回归模型和因变量自回归模型 马尔可夫模型 假设当前只跟少数数据相关，简化模型 潜变量模型 潜变量概括历史信息 文本预处理 主要预处理步骤：拆分为次元，建立词表，映射到数字索引 tokenize nlp 中最常见的操作 中文的分词使用 jieba 语言模型和数据集 目标：估计联合概率 面对问题：对文档或者词元序列进行建模 通过计数 常用统计方法：n 元语法，基于马尔可夫的统计模型 随机采样 基于随机的偏移量，不重叠、不相邻地划分序列 corpus, batch_size, num_steps：整个序列、小批量的大小，每一个子序列的长度 顺序分区 保证两个相邻的小批量中的子序列在原始序列上也是相邻的 循环神经网络 对于序列模型的神经网络，RNN 输出、隐变量、观察 隐变量和观察一起作为自变量，控制输出的隐变量 o_t ~ h_t H_t ~ h_t-1, x_t-1 拿掉 h_t-1 退化为 MLP 输出以及隐变量（的计算）在观察之前 观察是用于更新下一个单元，相当于观察既是输入也是标签 有点像因果系统（？），包含了时间信息 困惑度 衡量一个语言模型的好坏可以用平均交叉熵 困惑度取指数，来衡量，是平均每次可能选项 困惑度最好值是 1 梯度剪裁 RNN 的应用 文本生成：一个生多个 文本分类：多个生一个 问答、翻译：同时多个生成同时多个，有时间先后 Tag 生成：对每个词进行生成","categories":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"https://2incccc.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"d2l.ai","slug":"d2l-ai","permalink":"https://2incccc.github.io/tags/d2l-ai/"},{"name":"深度学习","slug":"深度学习","permalink":"https://2incccc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://2incccc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"视频原理","slug":"video","date":"2023-12-11T17:26:38.000Z","updated":"2025-02-20T09:35:58.870Z","comments":true,"path":"2023/12/12/video/","link":"","permalink":"https://2incccc.github.io/2023/12/12/video/","excerpt":"","text":"模拟视频基础 图像扫描 扫描：将图像编程顺序传送的电信号的过程，从左至右（行扫描）、从上至下（帧扫描?） 如何选择行频、帧频（场频）和行数 帧频：有连续感-无闪烁感：20-45.8Hz 行数：Z=620 行频 带宽：考虑扫描两个像素的时间的倒数，即为最高频率（带宽） 隔行扫描 减少传输带宽，每次扫描提供 1/2 的行数，每帧被分为两场（奇数/偶数） 优势：减少闪烁、降低传输带宽 缺点：行间闪烁、视在并行、图像边缘锯齿化等等 分解力 显示设备所能分辨的图像像素数目、客观指标。 视觉特性要求电视系统垂直和水平方向上所能分解的像素尺寸相同。 视频信号带宽 视频信号的带宽主要由扫描行数 Z 和帧频 fF 决定的,在行数 Z 一定时,可以改变 fF 而达到降低 Δf 的目的。 基带信号 兼容彩色电视系统 传输三基色：YUV 实现在黑白和彩色电视系统中均能正确传送演示 数字视频取样 数字化基础 三个步骤：采样、量化、编码 数字分量视频:ITU-R 601 标准 信号彩色空间转换 由 RGB-&gt;Y(R-Y)(B-Y)-&gt;YC_BC_R 采用 8bit 量化 取样频率 子取样格式 数字视频压缩技术 压缩方法：无失真（2-5 倍）有失真压缩（5-250 倍） 无失真方法：游程数据编码和变字长编码 有失真方法：量化、变化编码、DPCM 编码、运动估计和补偿 图像的相关性（冗余度） 空间冗余：相邻像素之间的相关性 时间冗余：前后帧的相似性 信息熵冗余 时间 X 的概率为 P(X) 信息量定义为 信息熵是对所有可能时间的信息量进行平均 码的冗余定义： 图像压缩编码 熵编码 无损编码、针对信息熵冗余 变字长编码原理：如果码字长度严格按照符号概率大小的相反顺序，得到的平均码字长度一定是最小的 Huffman 编码 合并：按照概率大小顺序排列，每次将概率最小的概率相加 置换：将合并吼的看成是一个新符号的概率 类推：重复上述做法，直到剩下两个符号概率 幅值和反推，每个分支分别赋 1,0 （可以概率大的赋 1，概率小的赋 0） 编码效率=信息熵/编码后平均长度 Huffman 码特点 三大缺点 码率变化，（变字场） 均匀分布的概率模型效率低 概率模型是可以变化的 两个优点 时间短 非歧义 算术编码 也是对概率大的符号赋予短码，但是编码过程和 Huffman 编码不同，且在信源概率分布均匀情况下编码效率更高。 Huffman 码是用整数长度的码字来编码的最佳方法,而算法编码是一种并不局限于整数长度码字的最佳编码方法。 原理：使用二进制，考虑累积概率。算法编码产生的码字实际上是一个二进制数值的指针，指向所编符号对应的概率区间(约定指向左端点) 编码基本法则 两个参量：编码点 C 和区间宽度 A 新原原区间新区间原区间 解码基本法则 采取和编码相反的步骤 从第一个区间开始，寻找字符串指向的子区间，得到第一个符号 减去原编码点除以区间宽度，得到新码字符串 之后符号以此类推 二进制算数编码 输入字符只有两种，如果信源字符集含有多个字符，先经过一些列二进判决，变成二进制字符串。和算术编码原理类似，不断划分概率子区间的递归过程 编码输出可以是最后一个编码区间中的任意数值，选择最短的比特长度。但是截断需要提供编码次数 率失真 率失真理论旨在寻求一种联系定长编码的失真度与编码数据率的方法。","categories":[],"tags":[{"name":"笔记","slug":"笔记","permalink":"https://2incccc.github.io/tags/%E7%AC%94%E8%AE%B0/"},{"name":"数字音视频","slug":"数字音视频","permalink":"https://2incccc.github.io/tags/%E6%95%B0%E5%AD%97%E9%9F%B3%E8%A7%86%E9%A2%91/"}]},{"title":"Rhythm, Tempo, and Beat Tracking","slug":"Rhythm-Tempo-and-Beat-Tracking","date":"2023-10-26T10:45:56.000Z","updated":"2025-02-20T09:51:48.406Z","comments":true,"path":"2023/10/26/Rhythm-Tempo-and-Beat-Tracking/","link":"","permalink":"https://2incccc.github.io/2023/10/26/Rhythm-Tempo-and-Beat-Tracking/","excerpt":"涵盖了关于节奏、节拍和音符起始点（onset）检测的关键概念和技术。","text":"Novelty Function 为了检测音符的开始，我们希望定位信号瞬态区域开始的突然变化，但是考虑到音高的变化不仅仅涉及响度的变化，也可能只涉及频率的改编（比如小提琴的演奏），所以，接下来我们介绍基于能量和基于频谱的 Novelty Function Energy-based Novelty Functions 通过均方根能量计算 主要涉及到的步骤：直接计算 RMSE 能量、求取 RMSE 的变化量（体现能量变化），对 delta RMSE 做半波整流，只保留能量增加部分 12345678910111213141516# 直接计算 RMSE 能量y_rms =librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length).flatten() # 计算差分rmse_diff = numpy.zeros_like(rmse)rmse_diff[1:] = numpy.diff(rmse) # 半波整流energy_novelty = numpy.max([numpy.zeros_like(rmse_diff), rmse_diff], axis=0) # 输出图像并比较plt.figure(figsize=(15, 6))plt.plot(t, rmse, &#x27;b--&#x27;, t, rmse_diff, &#x27;g--^&#x27;, t, energy_novelty, &#x27;r-&#x27;)plt.xlim(0, t.max())plt.xlabel(&#x27;Time (sec)&#x27;)plt.legend((&#x27;RMSE&#x27;, &#x27;delta RMSE&#x27;, &#x27;energy novelty&#x27;)) 对数能量 人类对声音强度的感知本质上是对数的。为了解释这一性质，我们可以在进行一阶差分之前对能量应用对数函数。 1log_rmse = numpy.log1p(10*rmse) 其余过程同上 Spectrum-based Novelty Functions 1spectral_novelty = librosa.onset.onset_strength(y=x, sr=sr) # 使用谱通量计算新奇函数 具体来说，librosa.onset.onset_strength 函数使用一种称为&quot;onset strength&quot; 的算法来计算这个信号。这个算法的目标是识别出音频信号中的突出事件，通常与音符、鼓击或其他音乐性事件的开始时刻相关 Peak computing 峰值计算 123456789101112def peak_pick(x, pre_max, post_max, pre_avg, post_avg, delta, wait): &#x27;&#x27;&#x27;Uses a flexible heuristic to pick peaks in a signal. A sample n is selected as a peak if the corresponding x[n] fulfills the following three conditions: 1. `x[n] == max(x[n - pre_max:n + post_max])` 2. `x[n] &gt;= mean(x[n - pre_avg:n + post_avg]) + delta` 3. `n - previous_n &gt; wait` where `previous_n` is the last sample picked as a peak (greedily). &#x27;&#x27;&#x27; 这是一个用于在信号中选择峰值（peaks）的函数，它采用了一种灵活的启发式方法。函数根据给定的条件选择信号中的峰值，这些条件旨在确保所选峰值具有一定的显著性和特定的时间间隔。让我解释每个条件的含义： x[n] == max(x[n - pre_max:n + post_max])：这表示在信号 x 中，一个样本 n 被选为峰值，如果它等于在前 pre_max 个样本和后 post_max 个样本范围内的样本中的最大值。这确保了所选的峰值是局部最大值。 x[n] &gt;= mean(x[n - pre_avg:n + post_avg]) + delta：这表示在信号 x 中，一个样本 n 被选为峰值，如果它的值大于或等于在前 pre_avg 个样本和后 post_avg 个样本范围内的样本的平均值再加上 delta。这个条件确保了所选的峰值比周围的平均值要显著。 n - previous_n &gt; wait：这个条件用于确保两个峰值之间有足够的时间间隔。 n 表示当前样本，previous_n 表示前一个已选择的峰值的样本。这个条件要求两个峰值之间的时间间隔至少为 wait 个样本。 因此，这个函数根据以上三个条件来选择信号中的峰值。这种方法可以用于从信号中提取出显著的峰值，例如，用于检测音频信号中的音符开始或其他信号中的突出事件。这种方法是一种启发式方法，可以根据应用的需求进行调整。 Onset detection (原理和上面类似) 12onset_frames = librosa.onset.onset_detect(y=x, sr=sr, wait=1, pre_avg=1, post_avg=1, pre_max=1, post_max=1)print(onset_frames) # frame numbers of estimated onsets 1[output]:[ 20 29 38 57 65 75 84 93 103 112 121 131 140 148 158 167 176 185 204 213 232 241 250 260 268 278 288] librosa.onset.onset_strength 和 librosa.onset.onset_detect 都是 Librosa 库中用于检测音频信号中的音符开始和强度的函数，但它们的功能和使用方式有一些不同。 librosa.onset.onset_strength: 功能：该函数计算音频信号的&quot;onset strength&quot; 或 “onset envelope”，即在时间上表示音频信号中的突出事件的信号。这个信号通常用于后续的音频事件检测。 参数：通常需要传递音频信号 y 和采样率 sr 作为参数，还可以提供其他参数来调整计算过程，例如 hop_length 和 aggregate. 返回值：函数返回一个代表音频信号强度的一维数组。 librosa.onset.onset_detect: 功能：这个函数使用 librosa.onset.onset_strength 的输出（或其他类似的音频强度信号）来检测音符开始或音频事件的时刻。它通过分析 “onset strength” 信号来查找潜在的音符开始时刻，并返回这些时刻的帧索引或时间。 参数：通常需要传递音频强度信号（如通过 librosa.onset.onset_strength 计算得到的）作为参数，以及一些其他参数，如 hop_length、backtrack 等，来调整检测过程。 返回值：函数返回一个包含音符开始时刻的帧索引或时间的一维数组。 总的来说，librosa.onset.onset_strength 用于计算音频信号的强度信号，而 librosa.onset.onset_detect 用于在强度信号上检测音符开始或音频事件的时刻。通常，它们一起使用，首先计算强度信号，然后使用 librosa.onset.onset_detect 来找到音符开始时刻。这种分离的方式允许更大的灵活性，因为您可以尝试不同的参数和强度信号来适应不同的音频分析任务。 Onset detection with backtracking 在很多情况中，考虑到音符改变不是瞬时完成，信号能量从平稳到峰值是有一个过程的，为避免检测 onset 时将 onset 从峰值切开，我们令参数 backtrack = True，实现对前面局部最值的回溯，可以确保能量变化的完整记录。 1onset_frames = librosa.onset.onset_detect(y=x, sr=sr, hop_length=hop_length, backtrack=True)","categories":[{"name":"Music Information Retrieval","slug":"Music-Information-Retrieval","permalink":"https://2incccc.github.io/categories/Music-Information-Retrieval/"}],"tags":[{"name":"音乐信息检索","slug":"音乐信息检索","permalink":"https://2incccc.github.io/tags/%E9%9F%B3%E4%B9%90%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"}]},{"title":"Signal Analysis and Feature Extraction","slug":"Signal-Analysis-and-Feature-Extraction","date":"2023-10-20T02:49:57.000Z","updated":"2025-02-20T09:51:48.406Z","comments":true,"path":"2023/10/20/Signal-Analysis-and-Feature-Extraction/","link":"","permalink":"https://2incccc.github.io/2023/10/20/Signal-Analysis-and-Feature-Extraction/","excerpt":"对音频信号的分析及相关特征提取","text":"基本特征提取 12345678910# 绘制波形librosa.display.waveshow([x[:1000]])# 特征提取：过零率/质心def extract_features(signal): stft_signal = librosa.core.stft(signal) magnitude = numpy.abs(stft_signal) return [ librosa.feature.zero_crossing_rate(signal)[0,0] librosa.feature.spectral_centroid(S=magnitude)[0,0] ] 零交叉率（Zero Crossing Rate）：零交叉率是一个表示信号快速变化的特征。它指的是信号波形穿过零轴的次数。在这段代码中，通过 librosa.feature.zero_crossing_rate(signal) [0, 0]来计算音频信号的零交叉率，并将结果作为特征之一返回。 频谱质心（Spectral Centroid）：频谱质心是频谱能量的加权平均值，用于表示音频信号的频谱中心。频谱质心越高，表示频谱的能量集中在较高的频率上，反之亦然。在这段代码中，通过 librosa.feature.spectral_centroid(S=magnitude) [0, 0]来计算音频信号的频谱质心，并将结果作为特征之一返回。 Feature Scaling 特征缩放 我们在上一个示例中使用的特征包括过零率和谱质心。这两个特征使用不同的单位来表示。这种差异可能会在稍后执行分类时带来问题。因此，我们将每个特征向量归一化到一个公共范围，并存储归一化参数以供以后使用。存在许多用于扩展功能的技术。现在，我们将使用[sklearn.preprocessing.MinMaxScaler]( http://scikit-learn.org/stable/modules/ generated/sklearn.preprocessing.MinMaxScaler.html)。 MinMaxScaler 返回一个缩放值数组，使得每个特征维度都在 -1 到 1 的范围内。 12345feature_table = numpy.vstack((kick_features, snare_features))scaler = sklearn.preprocessing.MinmaxScaler(feature_range=(-1,1)) # 定义一个范围在-1到1的预处理器training_features = scaler.fit_transform(feature_table) Segmantation 分割 在音频处理中，通常使用恒定的帧大小和跳跃大小（即增量）一次对一帧进行操作。帧的持续时间通常选择为 10 到 100 毫秒。 Segmentation Using Python List Comprehensions 在 Python 中，您可以使用标准的列表理解（https://docs.python.org/2/tutorial/datastructs.html#list-com经理）来执行信号分割并同时计算 RMSE。 123456# 定义帧长和间隔frame_length = 1024hop_length = 512# 均方根def RMSE(x): return numpy.sqrt(numpy.mean(x**2)) 给定一个信号，[librosa.util.frame]( https://librosa.github.io/librosa/ generated/librosa.util.frame.html #librosa .util.frame)将生成一个统一大小的帧列表: 1frames = librosa.util.frame(x, frame_length=frame_length, hop_length=hop_length) Energy 能量 信号的能量（[Wikipedia]( https://en.wikipedia.org/wiki/Energy_ (signal_processing%29); FMP, p. 66）对应于信号的总幅度。对于音频信号，大致对应于信号的响度。信号中的能量定义为 $$ \\sum_n \\left| x(n) \\right|^2 $$ The root-mean-square energy (RMSE) in a signal is defined as $$ \\sqrt{ \\frac{1}{N} \\sum_n \\left| x(n) \\right|^2 } $$ 123456789# 按照定义energy = numpy.array([ sum(abs(x[i:i+frame_length]**2)) for i in range(0, len(x), hop_length)])# 利用librosa函数rmse = librosa.feature.rmse(x, frame_length=frame_length, hop_length=hop_length, center=True) # shape(1,194)rmse = rmse[0] 1234567# 比较波形图和均方根能量frames = range(len(energy))t = librosa.frames_to_time(frames, sr=sr, hop_length=hop_length)librosa.display.waveshow(x, sr=sr, alpha=0.4)plt.plot(t, energy/energy.max(), &#x27;r--&#x27;) # normalized for visualizationplt.plot(t[:len(rmse)], rmse/rmse.max(), color=&#x27;g&#x27;) # normalized for visualizationplt.legend((&#x27;Energy&#x27;, &#x27;RMSE&#x27;)) Zero Crossing Rate 过零率 过零率指代信号波形穿过零轴的次数 12345678910n0 = 6500n1 = 7500plt.figure(figsize=(14, 5))plt.plot(x[n0:n1]) # zoom inzero_crossings = librosa.zero_crossings(x[n0:n1], pad=False) # 是否经过零点，输出结果为False和True的组合zeor_crossings.shape # output: (1000,0)zcrs = librosa.feature.zero_crossing_rate(x) # 过零率print(zcrs.shape) # output: (1,97) 过零率的高低与信号波形的特性有关。以下是一些常见的情况： 浊音/有谐波声音：浊音指的是声音中含有频谱中的多个谐波分量，通常听起来比较富有音色。浊音的过零率较低，因为在谐波声音中，波形会频繁穿过零线。 清音/无谐波声音：清音指的是声音中几乎没有谐波成分，通常听起来比较纯净。清音的过零率较高，因为在没有谐波的声音中，波形变化相对较平缓，不会频繁穿过零线。 静音：静音时，信号波形处于零线附近，过零率较高，因为信号在静音状态时频繁地从正值到负值或从负值到正值。 傅立叶变换 傅里叶变换(维基百科)是应用数学和信号处理中最基本的运算之一。 它将时域信号转换到频域。时域将信号表示为一系列采样，而频域将信号表示为不同幅度、频率和相位偏移的正弦波的叠加。 12345678910x,sr = librosa.load(&#x27;filename&#x27;) # 加载音频X = scipy.fft(x) # 求傅立叶变换X_mag = numpy.absolute(X) # 求模f = numpy.linspace(0, sr, len(X_mag)) # frequency variable 频率范围plt.figure(figsize=(13, 5))plt.plot(f, X_mag) # magnitude spectrumplt.xlabel(&#x27;Frequency (Hz)&#x27;) Short-Time Fourier Transform STFT 短时傅里叶变换 音乐信号是高度非平稳性的，也就是说，它们的统计数据会随着时间而变化。在一整首10分钟的歌曲中计算一次傅里叶变换是毫无意义的。 短时傅里叶变换(STFT)(维基百科;FMP，第 53 页)是通过计算信号中连续帧的傅里叶变换得到的。 $$ X(m, \\omega) = \\sum_n x(n) w(n-m) e^{-j \\omega n} $$ 当我们增加 $m$ 时，我们将窗口函数 $w$ 向右滑动。对于得到的坐标系，$x(n) w(n-m)$，我们计算傅里叶变换。因此，STFT $X$ 是时间 $m$ 和频率 $ω$ 的函数。 123hop_length = 512n_stft = 1024 # 设定STFT参数，包括帧长度和间隔X = librosa.stft(x, n_fft=n_fft, hop_length=hop_length) Spectrogram 谱图 在音乐处理中，我们通常只关心谱幅值而不关心相位含量。 谱图(维基百科;FMP(第 29、55 页)显示了频率随时间的强度。谱图就是 STFT 的平方幅度: $$ S(m, \\omega) = \\left| X(m, \\omega) \\right|^2 $$ 人类对声音强度的感知是基于对数（logarithmic）的，所以我们对对数幅度更感兴趣 123456S = librosa.amplitude_to_db(abs(X)) # 转化为对数plt.figure(figsize = (15,5))librosa.display.specshow(S, sr=sr, hop_length=hop_length, x_axis=&#x27;time&#x27;, y_axis=&#x27;linear&#x27;) # 使用specshow函数plt.colorbar(format=&#x27;%+2.0f dB&#x27;) Constant -Q Transform 常数 Q 变换 与傅立叶变换不同，但类似于 MEL 比例，常量 Q 变换(Wikipedia)使用对数间隔的频率轴。 Constant -Q Transform (CQT)是一种在频率上使用不同的频率分辨率来表示音频信号的方法，它模拟了人类听觉系统对不同音高的感知尺度。通过 CQT 变换，我们可以将音频信号转换为频谱表示，其中横轴表示时间，纵轴表示音高。 123fmin = librosa.midi_to_hz(36) # 设定最低频率C = librosa.cqt(x, sr=sr, fmin=fmin, n_bins=72) logC = librosa.amplitude_to_db(abs(C)) # 转换为对数谱 参数解释： fmin 是 CQT 变换的最低频率，表示变换时使用的最低音高。较低的 fmin 值将使 CQT 对低音更敏感。 n_bins 表示频率的总数量，它决定了 CQT 变换的音高范围和频率分辨率。N_bins 越大，音高范围越宽，频率分辨率越高。 Chroma (Chroma Vector) 色度向量(Wikipedia)(fmp，p.123)通常是12个元素的特征向量，指示信号中存在每个基音类别{C，C#，D，D#，E，…，B}的多少能量。 1234chromagram01 = librosa.feature.chroma_stft(y=x, sr=sr, hop_length=hop_length) # stft的色度向量 chromagram02 = librosa.feature.chroma_cqt(y=x, sr=sr, hop_length=hop_length) # cqt的色度向量plt.figure(figsize=(15, 5))librosa.display.specshow(chromagram, x_axis=&#x27;time&#x27;, y_axis=&#x27;chroma&#x27;, hop_length=hop_length, cmap=&#x27;coolwarm&#x27;) 输出结果： 色度能量归一化统计量(Chroma energy normalized statistics, CENS)。CENS 功能的主要思想是对大窗口进行统计，以平滑节奏、清晰度和音乐装饰(如颤音和弦)的局部偏差。CENS 最适合用于音频匹配和相似性等任务。librosa.feature.chroma_cens() Magnitude_scaling 振幅缩放（？） 通常，信号在时域或频域中的原始幅度与人类的感知相关性不如转换成其他单位的幅度，例如使用对数标度。 即使振幅呈指数增长，对我们来说，响度的增加似乎是渐进的。这种现象是 Weber-Fechner 定律(维基百科)的一个例子，该定律指出刺激和人类感知之间的关系是对数的。 Spectral Features 频谱特征 对于分类问题，我们将使用新的统计量矩（Moment）（包括质心、带宽、偏度、峰度）和其他谱统计数据。 矩（Moment）是物理学和统计学中出现的术语。矩的两个示例：均值和方差，第一个是原点矩，第二个是中心矩。 频谱质心 频谱质心（维基百科）指示频谱能量集中在哪个频率。这就像加权平均值： $$ f_c = \\frac{\\sum_k S(k) f(k)}{\\sum_k S(k)} $$ 其中 $S(k)$ 是频率 bin $ 处的频谱幅度 k$, $f(k)$ 是 bin $k$ 处的频率。 [librosa.Feature.Spectral_centroid]( https://librosa.github.io/librosa/ generated/librosa.Feature.Spectral_centroid.Html #librosa .feature.Spectral_centroid) 计算信号中每个帧的光谱质心. 输出图像： 与过零率类似，信号开始处的频谱质心存在虚假上升。这是因为开始时的静默幅度很小，高频成分有机会占主导地位。解决这个问题的一种方法是在计算光谱质心之前添加一个小常数，从而在安静部分将质心移向零。 频谱带宽 [librosa.feature.spectral_bandwidth]( https://librosa.github.io/librosa/ generated/librosa.feature.spectral_bandwidth.html #librosa .feature.spectral_bandwidth) 计算 $p$ 阶光谱带宽： $$ \\left( \\sum_k S(k) \\left(f(k) - f_c \\right)^p \\right)^{\\frac{1}{p}} $$ 其中 $S(k)$ 是在频率 $k$ 处的幅度，$f(k)$ 是 $k$ 处的频率，$f_c$ 是频谱质心。当 $p = 2$ 时，这就像加权标准差。 频谱对比度 考虑频谱峰值、谷值以及他们在每个频率子带中的差异。 [librosa.feature.spectral_contrast]( https://librosa.github.io/librosa/ generated/librosa.feature.spectral_contrast.html) 计算每个时间帧的六个子带的光谱对比度： 频谱滚降 是指低于总频谱能量指定百分比的频率 lirosa.feature.spectral_rolloff Autocorrelation 自相关 指代自身和时移后自身的相关性。对于信号 $x$，它的自相关信号 $r(k)$ 为 $$ r(k) = \\sum_n x(n) x(n-k) $$ 在此等式中，$k$ 通常称为 lag 参数。 $r(k)$ 在 $k = 0$ 处最大化，并且关于 $k$ 对称。 自相关对于查找信号中的重复模式很有用。例如，在短滞后时，自相关可以告诉我们有关信号基频的信息。对于较长的滞后，自相关可以告诉我们一些有关音乐信号节奏的信息。 两种计算 autororrelation 的方法：numpy.correlate 和 librosa.autocorrelation 音高估计 自相关用于查找信号内的重复模式。对于音乐信号，重复模式可以对应于音高周期。因此，我们可以使用自相关函数，通过寻找最值点来估计音乐信号中的音高。 Pitch Transcription Exercise 声调转录 在音频信号处理中，声调转录是指将音频中的音高信息转录成对应的音符或音高表示的过程。这个过程通常涉及到分析音频信号中的频率变化和音高轮廓，从而识别出其中的音符和音高变化。 准备工作： 导入库函数 1234%matpltlib inline # 将图像输出在notebook中而不是在新窗口import numpy, IPython.display as ipd, matpltlib.pyplot as pltimport librosa, librosa.displayplt.rcParams[&#x27;figure.figsize&#x27;] = (14, 5) 加载音频并播放 123filename = &#x27;../audio/simple_piano.wav&#x27;x,sr = librosa.load(filename)ipd.Audio(x,rate=sr) 计算 CQT 并输出频谱 12345bins_per_octave = 36 # 设置每个八度的频率间隔数目，表示频率轴分辨率cqt = librosa.cqt(x,sr=sr, n_bins=300, bins_per_octave = bins_per_octave) # 使用cqt函数log_cqt = librosa.amplitude_to_db(numpy.abs(cqt)) #转化对数谱librosa.display.specshow(log_cqt, sr=sr, x_axis=&#x27;time&#x27;, y_axis=&#x27;cqt_note&#x27;, bins_per_octave=36) 简单观察声谱图可知，整个音频大概包含八个相同或不同的音符，但是混杂在每一个音符的还有各种其他频率的分量。 任务目标 识别每个音符的音高，并将每个音符用相同音调的纯音（Pure Tone）组合起来代替音频 任务流程 第一步：检测起点 在音频信号处理和音乐分析中，“onset”（起点）是指音频信号中音乐或声音的开始部分，即音频信号开始出现显著能量变化的位置。换句话说，“onset” 表示音频信号中从无声到有声或从背景噪声到音乐开始的那个时间点。 在音频信号处理中，通常使用不同的算法和特征来检测 “onset”，比如短时能量、短时过零率、梅尔频率倒谱系数（MFCC）等。这些方法可以帮助准确地找到音频信号中显著的能量变化点，从而确定 “onset” 的位置。 在这里，我们使用新颖度函数（novelty function）来寻找音频信号中的起点。 1234hop_length = 100onset_env = librosa.onset.onset_strength(y=x, sr=sr, hop_length=hop_length)plt.plot(onset_env)plt.xlim(0,len(onset_env)) 在上图可以看到，除了几个比较显著的波峰，还有更多的很小的波峰，我们需要设置参数来忽略这些很小的波峰。 接下来使用 onset_detct 实现对起点的检测 1234567891011onset_samples = librosa.onset.onset_detect(y=x, sr=sr, units=&#x27;samples&#x27;, hop_length=hop_length, backtrack=False, pre_max=20, post_max=20, pre_avg=100, post_avg=100, delta=0.2, wait=0)print(onset_samples) 1output:[5800 11300 22300 33300 44300 55300 66400] 为了能将整个音频按照音符数分割开来，还要在序列首尾添加 padding 12onset_boundaries = numpy.concatenate([[0], onset_samples, [len(x)]])print(onset_boundaries) 1output:[0 5800 11300 22300 33300 44300 55300 66400 84928] 最后将采样点数转换为时间 12onset_times = librosa.samples_to_time(onset_boundaries,sr=sr)print(onset_times) 12output:array(array([ 0. , 0.26303855, 0.51247166, 1.01133787, 1.51020408,2.00907029, 2.50793651, 3.01133787, 3.85160998])) 最后将分割后的结果在频谱图中展示出来 12librosa.display.waveshow(x,sr=sr)plt.vlines(onset_times,-1,1,color=&#x27;r&#x27;) 经过上面的操作，我们可以看懂，红线将整个将音频中八个音符，对应波形中有明显不连续的地方分割开来。 第二步，估计音调 我们效仿前面的学习内容，使用自相关方法确定音高。 自相关用于查找信号内的重复模式。对于音乐信号，重复模式可以对应于音高周期。因此，我们可以使用自相关函数，通过寻找最值点来估计音乐信号中的音高 123456789101112131415def estimate_pitch(segment, sr, fmin=50.0, fmax=2000.0): # 计算输入的自相关 r = librosa.autocorrelate(segment) # 定义自相关最值点的范围 i_min = sr/fmax i_max = sr/fmin r[:int(i_min)] = 0 r[int(i_max):] = 0 # 寻找最值，返回对应频率 i = r.argmax() f0 = float(sr)/i return f0 第三步：生成纯音 Pure Tone 这里我们直接使用 numpy.sin 生成频率固定的正弦波纯音。 1234def generate_sine(f0, sr, n_duration): # 生成正弦波 n = numpy.arange(n_duration) return 0.2*numpy.sin(2*numpy.pi*f0*n/float(sr)) 第四步：将纯音组合起来 12345678910def estimate_pitch_and_generate_sine(x, onset_samples, i, sr): # 找到起点位置的频率，将每一音符分割开来 n0 = onset_samples[i] n1 = onset_samples[i+1] # 调用函数，估计每个音符的音高 f0 = estimate_pitch(x[n0:n1], sr) # 返回相同音高的纯音 return generate_sine(f0, sr, n1-n0) 接下来使用 numpy.concatenate 将合成的片段连接起来并演奏 12345y = numpy.concatenate([ estimate_pitch_and_generate_sine(x, onset_boundaries, i, sr=sr) for i in range(len(onset_boundaries)-1)])ipd.Audio(y,rate=sr) 为可视化展现合成后音频的最终结果，绘制合成后音频的 CQT 谱图 12cqt=librosa.cqt(y,sr=sr)librosa.display.specshow(abs(cqt),sr=sr,x_axis=&#x27;time&#x27;,y_axis=&#x27;cqt_mode&#x27;) 可以清晰看到，每个音符对应的频率谱图变得纯净，其他频率的分量基本完全消失。 至此，我们完成了这段音频的声调转录工作。","categories":[{"name":"Music Information Retrieval","slug":"Music-Information-Retrieval","permalink":"https://2incccc.github.io/categories/Music-Information-Retrieval/"}],"tags":[{"name":"音乐信息检索","slug":"音乐信息检索","permalink":"https://2incccc.github.io/tags/%E9%9F%B3%E4%B9%90%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"}]},{"title":"神经网络训练","slug":"neural-network-training","date":"2023-07-14T17:23:05.000Z","updated":"2025-02-20T09:51:48.412Z","comments":true,"path":"2023/07/15/neural-network-training/","link":"","permalink":"https://2incccc.github.io/2023/07/15/neural-network-training/","excerpt":"Chapter 4 神经网络训练 神经网络训练 训练的过程是反向传播的过程，利用得到的输出值与预测值的偏差，（损失函数），反向更新模型中的参数 逼近的思路理解训练 随机生成一个三阶函数，赋予一组随机参数，得到的输出与 sine 输出值比较，差值 loss 最小的那一组参数就为目标函数。 流程： 根据预测值和标签值得到 loss Loss 函数对各个参数反向求偏导 计算每个参数的梯度 更新参数值 梯度置 0 再次循环","text":"Chapter 4 神经网络训练 神经网络训练 训练的过程是反向传播的过程，利用得到的输出值与预测值的偏差，（损失函数），反向更新模型中的参数 逼近的思路理解训练 随机生成一个三阶函数，赋予一组随机参数，得到的输出与 sine 输出值比较，差值 loss 最小的那一组参数就为目标函数。 流程： 根据预测值和标签值得到 loss Loss 函数对各个参数反向求偏导 计算每个参数的梯度 更新参数值 梯度置 0 再次循环 反向传播 高阶函数构造 sin(x) c 常规思路：loss 对各参数求偏导，计算梯度，更新梯度值，梯度置 0 12345678910111213141516171819202122232425262728293031323334353637&quot;&quot;&quot;用一个三阶函数找到合适的参数 逼近y=sinx# 1.构建三阶函数# 2.给定输入，得到该函数的输出值,共循环500次# 3.得到该函数的输出值与y=sinx的输出 偏差loss函数# 4.为了得到loss最小，求该函数的极小值（导数）# 5.根据梯度值，更新参数&quot;&quot;&quot;import numpy as np# 0.sinex = np.linspace(start=-np.pi,stop=np.pi,num=2000)y = np.sin(x)a,b,c,d = np.random.rand(),np.random.rand(),np.random.rand(),np.random.rand()learning_rate = 1e-6# 学习率（learning rate）用于控制参数更新的步长。它决定了每一步更新中参数的变化量。for epoch in range(200000): y_pred = a + b*x + c*x**2 + d*x**3 loss = np.square(y_pred - y).sum() grad_y_pre = 2 * (y_pred - y) grad_a = grad_y_pre.sum() grad_b = (grad_y_pre * x**1).sum() grad_c = (grad_y_pre * x**2).sum() grad_d = (grad_y_pre * x**3).sum() a -= learning_rate * grad_a b -= learning_rate * grad_b c -= learning_rate * grad_c d -= learning_rate * grad_d # 以上为求导过程 # 根据链式法则，损失函数关于b的偏导数可以表示为：∂loss/∂b = ∂loss/∂y_pred * ∂y_pred/∂b if epoch%20 == 0: # Epoch（时期）是指将整个训练数据集（dataset）通过神经网络进行前向传播和反向传播的一次完整迭代。 print(loss)print(f&quot;y_pred = &#123;a.item()&#125; + &#123;b.item()&#125;*x + &#123;c.item()&#125;*x^2 + &#123;d.item()&#125;*x^3&quot;) loss.backward():由该函数确定更新后的参数值 （这是一个 PyTorch 库中的函数，输入输出需要为张量） 123456789101112131415161718192021222324252627282930313233343536373839&quot;&quot;&quot;用一个三阶函数找到合适的参数 逼近y=sinx# 1.构建三阶函数# 2.给定输入，得到该函数的输出值,共循环500次# 3.得到该函数的输出值与y=sinx的输出 偏差loss函数# 4.为了得到loss最小，求该函数的极小值（导数）# 5.根据梯度值，更新参数&quot;&quot;&quot;import torch# 0.sinex = torch.linspace(start=-torch.pi,end=torch.pi,steps=2000)y = torch.sin(x)a,b,c,d = torch.rand((),requires_grad=True),torch.rand((),requires_grad=True),\\ torch.rand((),requires_grad=True),torch.rand((),requires_grad=True)learning_rate = 1e-6for epoch in range(2000): y_pred = a + b*x + c*x**2 + d*x**3 loss = torch.square((y_pred - y),).sum() debug = 1 loss.backward() # 使用该函数，代替求导的过程 with torch.no_grad(): a -= learning_rate * a.grad b -= learning_rate * b.grad c -= learning_rate * c.grad d -= learning_rate * d.grad a.grad = None b.grad = None c.grad = None d.grad = None debug = 1 if epoch%20 == 0: print(loss) print(f&quot;y_pred = &#123;a.item()&#125; + &#123;b.item()&#125;*x + &#123;c.item()&#125;*x^2 + &#123;d.item()&#125;*x^3&quot;) optimiser.step(): 优化器函数（同上） 123456789101112131415161718192021222324252627282930313233343536373839404142434445&quot;&quot;&quot;用一个网络模型 逼近y=sinx# 1.给定输入，得到sin的输出值为y# 2.给定输入，根据y=a+b*x+c*x**2+d*x**3,计算^1,^2,^3不同幂次下的结果# 3.构建网络模型，利用线性层将不同幂次下的结果按一定权重相加，包含线性层Linear(3,1),Flatten()# 4.将三个结果放入模型得到该函数的输出值,共循环500-&gt;2000次# 4.得到该函数的输出值与y=sinx的输出,偏差loss函数= torch.square(y_pre - y).sum()# 5.为了得到loss最小，求该函数的极小值（导数）loss.backward()# 6.根据梯度值，更新参数 param -= learning_rate * param.grad 之后 model.zero_grad()&quot;&quot;&quot;from torch import nnimport torchclass Liner(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential(nn.Linear(3,1),nn.Flatten(0,1)) def forward(self,data): output = self.model(data) return outputx = torch.linspace(-torch.pi,torch.pi,2_000)y = torch.sin(x)# x^1,x^2,x^3mynn = Liner()p = torch.tensor([1,2,3])input = x.unsqueeze(-1).pow(p)learning_rate = 1e-4optimiser = torch.optim.RMSprop(params=mynn.model.parameters(),lr=learning_rate)loss_fn = torch.nn.MSELoss() # 代替square平方求和for epoch in range(2_000): y_pre = mynn(input) loss = loss_fn(y_pre,y) layer_liner = mynn.model[0] layer_flatten = mynn.model[1] debug = 1 optimiser.zero_grad() # 用于将模型参数的梯度归零。 loss.backward() # 反向更新参数 optimiser.step() # 优化器，代替-=的过程 if epoch % 10==0: print(loss) debug = 1debug = 1 需要注意： optimiser = torch.optim.RMSprop(params=mynn.model.parameters(),lr=learning_rate) 定义了一个优化器，参数是模型中的各个参数，lr 是学习率。在机器学习和深度学习中，优化器（Optimizer）是一种用于调整模型参数以最小化损失函数的算法或方法。优化器根据模型的梯度信息和指定的优化算法，更新模型参数的值，以便使损失函数达到最小值或接近最小值。在训练神经网络模型的过程中，优化器的作用非常重要。它能够根据损失函数的梯度信息来更新模型参数，使得模型能够逐步调整自身以更好地拟合训练数据。 loss.Backward() 的任务是执行反向传播计算梯度。具体来说，它计算损失函数 loss 关于模型参数的梯度，通过使用链式法则将梯度从损失函数传播到模型的每个参数。这样可以获得每个参数相对于损失函数的梯度信息，即参数的更新方向和大小。 optimiser.step() 的任务是根据梯度信息更新模型参数的值。它使用优化算法（如 RMSprop）和学习率来计算参数的更新量，并将这个更新量应用到模型的参数上，从而更新参数的值。这样，模型的参数会朝着减小损失函数的方向进行调整。 损失函数与优化器 常用的损失函数 平方损失 输出-预期的平方的求和 最大似然处理，输出的结果（似然值）视为概率，再去求得到该结果概率值最大的权重系数 w。已知事情发生的结果，反推发生该结果概率最大的参数 w P(x|w,b) 交叉熵损失 1234567891011121314151617181920&quot;&quot;&quot;损失函数的使用# 1.定义两个变量# 2.损失函数选择L1Loss()，参量选择 均值与取和——(P1-E1)+(P2-E2)+...(PN-EN)/N# 3.损失函数选择MSELoss()——(P1-E1)^2+(P2-E2)^2+...(PN-EN)^2/N&quot;&quot;&quot;import torchfrom torch import nny_pred = torch.tensor([1,2,3],dtype=torch.float32)y = torch.tensor([1,2,5],dtype=torch.float32)# 2.损失函数选择L1Loss()，参量选择 均值与取和——(P1-E1)+(P2-E2)+...(PN-EN)/Nloss_l1 = torch.nn.L1Loss(reduction=&quot;sum&quot;)result1 = loss_l1(y_pred,y)print(result1)# 3.损失函数选择MSELoss()——(P1-E1)^2+(P2-E2)^2+...(PN-EN)^2/Nloss_mse = torch.nn.MSELoss(reduction=&quot;sum&quot;)result2 = loss_mse(y_pred,y)print(result2) 常用的优化器 SGD Adam 构建神经网络全过程 搭建+训练（Chapter 3+4） 下载数据-&gt;加载数据-&gt;准备模型-&gt;设置损失函数-&gt;设置优化器-&gt;开始训练-&gt;最后验证-&gt;结果聚合展示 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475from torch import nnimport torch# 1.搭建模型class Mynetwork(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential(nn.Conv2d(3,32,5,1,2),nn.MaxPool2d(2), nn.Conv2d(32,32,5,1,2),nn.MaxPool2d(2), nn.Conv2d(32,64,5,1,2),nn.MaxPool2d(2), nn.Flatten(),nn.Linear(64*4*4,64),nn.Linear(64,10)) # 包括卷积、池化、线性等等 def forward(self,data): # 前向驱动函数 output = self.model(data) return output# 2.得到数据集input = torch.ones(size=(1,3,32,32),dtype=torch.float32) # 数据集y = torch.tensor([[1,2,3,4,5,6,7,8,9,10]],dtype=torch.float32)# 3.调用模型得到输出mynn = Mynetwork()loss_fn = torch.nn.MSELoss(reduction=&quot;mean&quot;) # 损失函数optimiser = torch.optim.RMSprop(params=mynn.model.parameters(),lr=1e-4) # 定义优化器for period in range(100): print(f&quot;this is period &#123;period+1&#125;:&quot;) for data in range(1): y_pred = mynn(input) # 导入模型 # 4.计算真实值与输出值之间的偏差loss loss = loss_fn(y_pred,y) # 7.迭代一次后 梯度置零 optimiser.zero_grad() # 5.计算各参量的梯度值 loss.backward() # 6.用优化器更新参数 optimiser.step() if data % 10 ==0: print(loss)print(f&quot;The final outcome is &#123;y_pred&#125;&quot;)----------------------------------------------------# output：this is period 1:tensor(38.6425, grad_fn=&lt;MseLossBackward0&gt;)this is period 2:tensor(34.2981, grad_fn=&lt;MseLossBackward0&gt;)this is period 3:tensor(22.4355, grad_fn=&lt;MseLossBackward0&gt;)this is period 4:tensor(3.7360, grad_fn=&lt;MseLossBackward0&gt;)this is period 5:tensor(4.2519, grad_fn=&lt;MseLossBackward0&gt;)this is period 6:tensor(7.7438, grad_fn=&lt;MseLossBackward0&gt;)this is period 7:....tensor(0.1047, grad_fn=&lt;MseLossBackward0&gt;)this is period 96:tensor(0.1212, grad_fn=&lt;MseLossBackward0&gt;)this is period 97:tensor(0.0982, grad_fn=&lt;MseLossBackward0&gt;)this is period 98:tensor(0.1128, grad_fn=&lt;MseLossBackward0&gt;)this is period 99:tensor(0.0920, grad_fn=&lt;MseLossBackward0&gt;)this is period 100:tensor(0.1051, grad_fn=&lt;MseLossBackward0&gt;)The final outcome is tensor([[0.8946, 1.8333, 2.8251, 3.7509, 4.7626, 5.6981, 6.6488, 7.6014, 8.5054, 9.5048]], grad_fn=&lt;AddmmBackward0&gt;) Tensor(38.6425)：这部分表示损失函数的数值，即计算得到的具体损失值。在这个例子中，损失函数的值为 38.6425。 grad_fn=&lt;MseLossBackward0&gt;：这部分表示损失函数的计算图中的反向传播函数。它指示了该张量是通过执行反向传播操作计算得到的，并且在计算图中有一个与之相关的反向传播函数。在这个例子中，使用的是均方误差损失函数（MSELoss），因此显示为 &lt;MseLossBackward0&gt;。","categories":[{"name":"深度学习入门","slug":"深度学习入门","permalink":"https://2incccc.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://2incccc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"音频信号","slug":"音频信号","permalink":"https://2incccc.github.io/tags/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7/"},{"name":"声学","slug":"声学","permalink":"https://2incccc.github.io/tags/%E5%A3%B0%E5%AD%A6/"},{"name":"神经网络","slug":"神经网络","permalink":"https://2incccc.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"信号处理","slug":"信号处理","permalink":"https://2incccc.github.io/tags/%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"},{"name":"人工智能","slug":"人工智能","permalink":"https://2incccc.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}]},{"title":"【大唐杯】5G网络架构与组网部署","slug":"DT-5g-network","date":"2023-02-06T14:13:59.000Z","updated":"2025-02-20T09:35:08.108Z","comments":true,"path":"2023/02/06/DT-5g-network/","link":"","permalink":"https://2incccc.github.io/2023/02/06/DT-5g-network/","excerpt":"课程来源：第一章-5G网络架构与组网部署-01 课程目标 5G网络整体架构组成 主要网元功能 网元间接口关系 了解5G网络组网部署策略 目录 1.1 5G网络架构的演进趋势 1.2 5G网元功能与接口 1.3 5G网络组网部署 1.1 5G网络架构的演进趋势","text":"课程来源：第一章-5G网络架构与组网部署-01 课程目标 5G网络整体架构组成 主要网元功能 网元间接口关系 了解5G网络组网部署策略 目录 1.1 5G网络架构的演进趋势 1.2 5G网元功能与接口 1.3 5G网络组网部署 1.1 5G网络架构的演进趋势 概述 5G通信系统包括 5GC(5G Core Network) 和 NG-RAN(Next Generation Radio Access Network) NG接口链接核心网和接入网，实现控制面和用户面功能； Xn接口链接接入网，实现控制面和用户面功能。 接口为逻辑接口 gNB:5G基站 ng-eNB:增强4G基站 AMF/UPF/SMF 核心网网元 4G移动通信系统包括EPC(Evolved Packet Core network),演进分组核心网和E-UTRAN(Evolved Universal Terrestrial Radio Access Network)演进通用陆地无线接入网络 S1 X2 接口 5G 4G系统整体架构类似，区别如下： RAN网络引入 CU DU 组网灵活 MEC(Multi-access Edge Connection)多接入边缘计算是5G系统运行的关键技术，可实现5GC的部分功能，可将核心网部署在靠近基站的地方，降低时延。 关于前传中传回传 一个基站，通常包括BBU(Building Base band Unit，基带单元，主要负责信号调制)、RRU(Remote Radio Unit，主要负责射频处理)，馈线(连接RRU和天线)，天线(主要负责线缆上导行波和空气中空间波之间的转换)。4G每个基站都有一个BBU，并通过BBU直接连到核心网。 而在5G网络中，接入网不再是由BBU、RRU、天线这些东西组成了。而是被重构为以下3个功能实体： CU(Centralized Unit，集中单元)，DU(Distribute Unit，分布单元)，AAU(Active Antenna Unit，有源天线单元)。 原来4G的RRU和天线合并成AAU（方便大规模天线的实现），把BBU分离成CU和DU，DU下沉（见图1-3）到AAU处，一个CU可以连接多个DU。 4G只有前传和回传两部分，在5G网络中则演变为三个部分，AAU连接DU部分称为5G前传（Fronthaul），中传（Middlehaul）指DU连接CU部分，而回传（Backhaul）是CU和核心网之间的通信承载。 1.1.1 核心网架构演进 模拟通信：保密性差 数字通信：数字化，2.5G后可上网阶段 互联网：IP化，传输媒介发生改变，网线、光纤投入使用，设备围绕IP 端口进行，承载控制分离，网元功能细化。网业分离，分为控制面、用户面。3/4G阶段 SDN/NFV Software-defined Networking 软件定义网络 Network Functions Virtualization网络功能虚拟化， 网络架构颠覆，基于服务的网络架构，网元数量大量增加，UPF只用于 处理，控制处理分离，控制和用户完全的分离，软件硬件分离架构灵活 网元虚拟化易于操作 总结：模块化，虚拟化 4G核心网架构 各模块基于NFC实现虚线框内，为控制面 UPF为用户面，实现用户面控制面分离。 各网元功能见下节 1.1.2 无线接入网演进 “分合分”的表象 CU对实时性要求不高，实时性要求高在DU实现。 1.2 5G网元功能与接口 1.2.1 5G移动通信整体网络架构 网络功能间的信息交互基于两种方式表示：服务表示（模块名称前+N，指对外暴露的接口，多对一接口，用到服务注册和服务发现的功能，相互之间不需要知道功能所在的地址）、点对点表示（不同功能实体之间有约定好的接口，比较简单，不考虑注册和发现，但是拓展性弱）。 点对点表示如下图1-9 5GC各网元功能介绍 AMF Access and Mobility Management Function 接入和移动性管理功能 SMF Session Management function 会话管理功能 AUSF Authentication Server Function 认证服务器功能 UPF The User plane function 用户面功能 PCF Policy Control function 策略控制功能 UDM The Unified Data Management 统一数据管理功能 NRF NF Repository Function 网元存储功能 NSSF The Network Slice Selection Function 网络切片选择 NEF Network Exposure Function 网络开放功能 CU DU分离逻辑图： 层与层之间的交互： CU分为 CU-C C控制 CU-U U用户 内部接口： F1-C F1-U，对外接口Xn-C Xn-U 上图不代表实际连接情况，不等于gNB等于CU+DU 具体连接关系可调节。 CU DU有八种划分方式 CU便于集中化管理，DU便于更大传输带宽，更低时延。 3GPP(3rd Generation Partnership Project)标准确定了option2 1.2.2 5G主要网元功能 主要功能如下图： UPF（用户面功能） 掌握主要功能： gNB切换的本地移动锚点（适用时）：在不同地方使用网络确保连接连通，切换前后保持不变 连接到移动通信网络的外部PDU会话点 N接口切换过程中，数据匹配路径，路由与转发 Uplink流量验证（SDF到QoS流映射） SMF（会话管理功能） 终端发起寻呼，接入网可以响应，进行会话的建立 终端IP地址的分配和管理 选择合适的UPF 基于策略控制用户面功能 AMF（访问和移动性管理功能） NAS信令的加密和保护 注册管理 在UE和SMF直接传输SM消息，透传信息 gNB/en-gNB CU-C (Central Unit Control plane) 不同接口的管理和数据处理 连接管理包括：单连接 双连接 多链接 和D2D 系统内和系统间负载均衡 切片资源动态管理 CU-U 数据包的处理和转换 DU 资源调度、传输模式的转换、信道映射 AAU-RF(RAdio Frequency) 信号手法 Massive MIMO 大规模天线处理 频率时间同步 AAS实现机制 1.2.3 接口协议及功能 数据传输需遵循各个协议的要求，下面是主要接口 NG接口是接入网和核心网之间的接口，控制面和用户面分离 NG接口控制面功能流程描述 NG-U接口主要功能：用户面数据传送 Xn接口是基站之间的接口，分为控制面用户面 Xn-C是CU-C之间的接口，Xn-U是CU-U之间的接口 Xn-C接口功能流程描述 Xn-U主要功能 E1接口指CU-C与CU-U接口，只有控制面接口，支持信令信息的交换 F1接口是CU与DU之间的接口，支持信令交互，包括不同eNB-point的数据发送，包括控制面用户面 终端和基站之间的Uu接口 控制面：涉及终端、基站、核心网 NAS层属于控制面功能 用户面： 新的协议层SDAP层：业务适配层，完成流到无线承载的QoS映射，为每个报文打上流表示 1.3 5G网络组网部署 1.3.1 SA组网和NSA组网 NSA(Not standalon)：非独立：终端同一时间同时连接4g 5g基站 接入4g或5g的核心网 SA(standalone)：独立 区分根本不同：同一时间5g基站能否单独提供服务 原因：5g在刚刚引入时基站数量不足 SA组网方案：option2/5 option2:5gc–gNB option5:5gc–ng-eNB NAS组网部署： option3:4g 5g 基站合用4g核心网，控制面仅经由enb连接到epc，优势在于不必新增5G核心网，缺点是4g核心网有信令过载风险，该阶段主要解决初期的5g覆盖 option7:核心网变为5g核心网，控制网由ngenb连接到5gc，解决了4g核心网信令过载风险，主要面向5g容量需求 基站间接口变为Xn option4:控制面由gnb连接到5gc，该阶段不仅面向5g的增强型移动带宽场景（eMBB） 大规模物联网（mMTC）和低时延高可靠连接（URLLC），是面向万物连接时代5G的多样化业务 Ultra-Reliable and Low Latency Communications–URLLC Massive MachineType Communication–mMTC Enhanced Mobile Broadband–eMBB option4/7 不常用 5G核心网主要使用独立组网 3GPP协议下对基站定义： eNB 面向终端提供 E-UTRAN用户面控制面协议，通过S1接口连接EPC（4g核心网） ng-eNB 面向终端提供 E-UTRAN用户面控制面协议，通过NG接口连接5GC（5G核心网） gNB 面向终端提供NR用户面和控制面协议，通过NG接口连接到5GC en-gNB 面向终端提供NR用户面和控制面协议，通过S1-U接口连接到EPC的网络节点 SA NSA组网方案对比 1.3.2 MR-DC技术 Multi-RAT Dual Connectivity 多接入网技术双连接 一部终端可以同时连接4G 5G网络，同时使用两个网络进行业务，此时终端需要具备至少两个MAC实体，支持双发双收。 控制面协议栈 MN为主节点，SN辅节点，各自有RRC实体，可以生成要发送到终端的PDU，只有主节点才能连接到核心网 用户面承载概念 MCG(Master Cell Group):主小区组，和主节点相关呃校区 MCG承载的RLC实体一定落在主节点 SCG(Secondary Cell Group):辅小区组，和辅节点相关 SCG承载的RLC实体落在辅节点 分离承载：RLC实体既可以存在于主节点也可以存在于辅节点 承载可理解为用户面传递数据概念，从核心网数据经用户面传递的路径 有了双连接的概念，就有了MCG和SCG的概念。从信令交互角度来看，UE首先发起随机接入过程的小区（Cell）所在的组（Group）就是MCG。假若5G NR基站和LTE基站一起给UE提供双连接服务，LTE作主基站，5G NR基站作辅基站，那么LTE所提供的多个小区就是MCG（Master CellGroup，主小区组），5G NR提供的多个小区就是SCG（Secondary Cell Group，辅小区组）。MCG的小区和SCG的小区应该配置成邻小区关系。 5G组网MCG与SCG CA(Carrier Aggregation) 载波聚合 终端也与多个接入网网元连接，但是控制面连接仅有一个 5G NR协议栈 1.3.3 CU/DU组网部署 CU DU AAU三级配置可搭配处不同网络结构 为支持eMBB的覆盖和容量需求，CU DU 分离部署，分为Macro(宏)和Micro(微)方式 分离部署 两种方式相同 合设部署 DU+RRU-微组网部署 在密集部署条件下，联合多个DU形成基带池（时效性好），提高网络覆盖和容量，组网方式如下图 DU CU一起部署-&gt;大带宽低时延：视频、虚拟现实 DU CU分离-&gt;对带宽时延要求不高：语音业务 mMTC 缩略词解释","categories":[],"tags":[{"name":"5G","slug":"5G","permalink":"https://2incccc.github.io/tags/5G/"},{"name":"通信","slug":"通信","permalink":"https://2incccc.github.io/tags/%E9%80%9A%E4%BF%A1/"},{"name":"大唐杯","slug":"大唐杯","permalink":"https://2incccc.github.io/tags/%E5%A4%A7%E5%94%90%E6%9D%AF/"}]}],"categories":[{"name":"juce开发","slug":"juce开发","permalink":"https://2incccc.github.io/categories/juce%E5%BC%80%E5%8F%91/"},{"name":"深度学习入门","slug":"深度学习入门","permalink":"https://2incccc.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/"},{"name":"Music Information Retrieval","slug":"Music-Information-Retrieval","permalink":"https://2incccc.github.io/categories/Music-Information-Retrieval/"}],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"https://2incccc.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"OSPF","slug":"OSPF","permalink":"https://2incccc.github.io/tags/OSPF/"},{"name":"算力路由","slug":"算力路由","permalink":"https://2incccc.github.io/tags/%E7%AE%97%E5%8A%9B%E8%B7%AF%E7%94%B1/"},{"name":"juce","slug":"juce","permalink":"https://2incccc.github.io/tags/juce/"},{"name":"cmake","slug":"cmake","permalink":"https://2incccc.github.io/tags/cmake/"},{"name":"工具","slug":"工具","permalink":"https://2incccc.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"效率","slug":"效率","permalink":"https://2incccc.github.io/tags/%E6%95%88%E7%8E%87/"},{"name":"经验分享","slug":"经验分享","permalink":"https://2incccc.github.io/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/"},{"name":"论文","slug":"论文","permalink":"https://2incccc.github.io/tags/%E8%AE%BA%E6%96%87/"},{"name":"笔记","slug":"笔记","permalink":"https://2incccc.github.io/tags/%E7%AC%94%E8%AE%B0/"},{"name":"Dance Generation","slug":"Dance-Generation","permalink":"https://2incccc.github.io/tags/Dance-Generation/"},{"name":"Motion Generation","slug":"Motion-Generation","permalink":"https://2incccc.github.io/tags/Motion-Generation/"},{"name":"d2l.ai","slug":"d2l-ai","permalink":"https://2incccc.github.io/tags/d2l-ai/"},{"name":"深度学习","slug":"深度学习","permalink":"https://2incccc.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://2incccc.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"数字音视频","slug":"数字音视频","permalink":"https://2incccc.github.io/tags/%E6%95%B0%E5%AD%97%E9%9F%B3%E8%A7%86%E9%A2%91/"},{"name":"音乐信息检索","slug":"音乐信息检索","permalink":"https://2incccc.github.io/tags/%E9%9F%B3%E4%B9%90%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2/"},{"name":"音频信号","slug":"音频信号","permalink":"https://2incccc.github.io/tags/%E9%9F%B3%E9%A2%91%E4%BF%A1%E5%8F%B7/"},{"name":"声学","slug":"声学","permalink":"https://2incccc.github.io/tags/%E5%A3%B0%E5%AD%A6/"},{"name":"神经网络","slug":"神经网络","permalink":"https://2incccc.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"信号处理","slug":"信号处理","permalink":"https://2incccc.github.io/tags/%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"},{"name":"人工智能","slug":"人工智能","permalink":"https://2incccc.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"5G","slug":"5G","permalink":"https://2incccc.github.io/tags/5G/"},{"name":"通信","slug":"通信","permalink":"https://2incccc.github.io/tags/%E9%80%9A%E4%BF%A1/"},{"name":"大唐杯","slug":"大唐杯","permalink":"https://2incccc.github.io/tags/%E5%A4%A7%E5%94%90%E6%9D%AF/"}]}